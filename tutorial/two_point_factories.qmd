---
title: "Using Firecrown Factories to Initialize Two-Point Objects"
format: html
---

{{< include _functions.qmd >}}

## Purpose of this Document

In the tutorial [Two Point Generators](two_point_generators.qmd) we show how to use generators to create the necessary metadata to instantiate `TwoPoint` objects.
In addition to generators, the user has also the option to use data already saved in a `SACC` object (or file) to initialize the `TwoPoint` objects.
In this tutorial we show how to extract the metadata and data from `SACC` object and use the metadata and a factory to initialize the objects.

## `SACC` objects

The `SACC` objects are used to store all necessary information for a statistical analysis. 
In practice, it stores metadata (data layout, data type, bins, dependent variables, etc), calibration data (redshift distribution per bin $\mathrm{d}n/\mathrm{d}z$) and the covariance between all measurements. 
In the tutorials [`InferredGalaxyZDist`](inferred_zdist.qmd), [`InferredGalaxyZDist` Generators](inferred_zdist_generators.qmd) and [`InferredGalaxyZDist` Serialization](inferred_zdist_serialization.qmd) we show how to use, generate and serialize the `InferredGalaxyZDist` objects describing the redshift distributions. In this document we extract these components from a `SACC` object.

## Metadata only `SACC`

Up to the version 1.7 Firecrown relied on a two phase construction for its likelihoods.
During the first phase, the `Likelihood`, `Statistics` and `Systematics` objects are created with metadata only.
Then, the user must call the `read` method passing a `SACC` object to finish the object construction.

One shortcoming of this methodology is that the user must know the `SACC` object struct beforehand, so they can create the necessary objects.
Then, a matching `SACC` object must be passed in the `read` phase.
To simplify this, we introduced functions to extract the metadata from the `SACC` file, such that it can be used together with the factories to create the objects.

In the code below we extract the metadata indices from the `examples/des_y1_3x2pt` `SACC` file.
```{python}
from firecrown.metadata_functions import extract_all_real_metadata_indices
import sacc

sacc_data = sacc.Sacc.load_fits("../examples/des_y1_3x2pt/sacc_data.fits")

all_meta = extract_all_real_metadata_indices(sacc_data)
```

The metadata can be seen below:
```{python}
# | code-fold: true
import yaml
from IPython.display import Markdown

all_meta_prune = [
    {
        "data_type": meta["data_type"],
        "tracer1": str(meta["tracer_names"].name1),
        "tracer2": str(meta["tracer_names"].name2),
    }
    for meta in all_meta
]

all_meta_yaml = yaml.safe_dump(all_meta_prune, default_flow_style=False)

Markdown(f"```yaml\n{all_meta_yaml}\n```")
```

The tracer names above are ones from the `SACC` object. 
The user needs to know the tracer names to create the `TwoPoint` objects.
However, now the user can use the metadata to create the objects.

### Factories

The factories are used to create the `TwoPoint` objects.
In Firecrown, we write factories using Pydantic, a library to validate and parse data.
This gives us the advantage of having a schema for the data, and the user can use the factories to validate the data before creating the objects.

For example, we can organize the factories in a YAML file:
```{python}

from firecrown.likelihood.two_point import TwoPoint, TwoPointFactory
from firecrown.utils import (
    base_model_from_yaml,
    ClIntegrationOptions,
    ClIntegrationMethod,
    ClLimberMethod,
)

two_point_yaml = """
correlation_space: real
weak_lensing_factories:
  - type_source: default
    per_bin_systematics:
    - type: MultiplicativeShearBiasFactory
    - type: PhotoZShiftFactory
    global_systematics:
    - type: LinearAlignmentSystematicFactory
      alphag: 1.0
number_counts_factories:
  - type_source: default
    per_bin_systematics:
    - type: PhotoZShiftFactory
    global_systematics: []
"""

tp_factory = base_model_from_yaml(TwoPointFactory, two_point_yaml)
two_point_list = TwoPoint.from_metadata_index(all_meta, tp_factory)
```

### Creating a `Likelihood` object

Now we can create a `Likelihood` object using the `TwoPoint` objects.
Since we are using the metadata only, we need to pass the `SACC` object to the `read` method.
```{python}
from firecrown.likelihood.gaussian import ConstGaussian

likelihood = ConstGaussian(two_point_list)

likelihood.read(sacc_data)

```

## Extracting the Metadata and Data

In the previous section, we demonstrated how to extract metadata from a `SACC object`. 
Now, we will show how to extract all components from the `SACC` object, including the metadata, data, and calibration data.

In the upcoming interface for Firecrown, likelihood objects will be created using the complete data. 
This data may be extracted from a `SACC` object, generated by a custom generator, or provided in any other manner by the user.

In the code below we extract all components from the `examples/des_y1_3x2pt` `SACC` file.
```{python}
from firecrown.data_functions import (
    extract_all_real_data,
    check_two_point_consistence_real,
)

two_point_reals = extract_all_real_data(sacc_data)
check_two_point_consistence_real(two_point_reals)
```

The list `two_point_reals` contains all information about real-space two point functions contained in the `SACC` object.
Therefore, we need to use a different constructor to obtain the two-point objects already in the `ready` state.
```{python}

tp_factory = base_model_from_yaml(TwoPointFactory, two_point_yaml)
two_points_ready = TwoPoint.from_measurement(two_point_reals, tp_factory)
```
Note that we used the same factories to instantiate our `TwoPoint` objects. 

Now, to create the likelihood we need to use a different constructor that support creating likelihoods in `ready` state.
```{python}
from firecrown.likelihood.gaussian import ConstGaussian

likelihood_ready = ConstGaussian.create_ready(
    two_points_ready, sacc_data.covariance.dense
)
```
Note that, since we are creating a `Likelihood` in ready state, the constructor requires the statistics covariance.

## Comparing results

We can now compare whether the two methods yield the same results.
Before we do that, we need to obtain our required parameters.
```{python}
from firecrown.parameters import ParamsMap

req_params = likelihood.required_parameters()
req_params_ready = likelihood_ready.required_parameters()

assert req_params_ready == req_params

default_values = req_params.get_default_values()
params = ParamsMap(default_values)

```
Note that the required parameters are the same for both likelihoods.
Before generating the two-point statistics, we use the following default values:
```{python}
# | code-fold: true
import yaml

default_values_yaml = yaml.dump(default_values, default_flow_style=False)

Markdown(f"```yaml\n{default_values_yaml}\n```")
```


Finally, we can prepare both likelihoods and compare the loglike values.
```{python}
from firecrown.modeling_tools import ModelingTools
from firecrown.ccl_factory import CCLFactory
from firecrown.updatable import get_default_params_map
from firecrown.parameters import ParamsMap
from firecrown.utils import base_model_to_yaml
from firecrown.data_functions import TwoPointBinFilterCollection, TwoPointBinFilter
from firecrown.metadata_types import Galaxies


tools = ModelingTools(ccl_factory=CCLFactory(require_nonlinear_pk=True))
params = get_default_params_map(tools, likelihood)

tools = ModelingTools()
tools.update(params)
tools.prepare()

likelihood.update(params)
likelihood_ready.update(params)
```
```{python}
# | code-fold: true
print(f"Loglike from metadata only: {likelihood.compute_loglike(tools)}")
print(f"Loglike from ready state: {likelihood_ready.compute_loglike(tools)}")
```

## Filtering Data: Scale-cuts

Real analyses use only a subset of the measured two-points statistics, where the utilized data is typically limited my the accuracy of the models used to fit the data. 
It is then useful to define the physical scales (corresponding to the data) that should be analyzed in a given likelihood evaluation of two-point statistics. 
Firecrown can implement this feature though its factories, notably by defining a `TwoPointBinFilterCollection` object. 
This object is a collection of `TwoPointBinFilter` objects, which define the valid data analysis range for a given combination of two-point tracers. 
For instance, we can define the filtered range of galaxy clustering auto-correlations as follows:

```{python}
tp_collection = TwoPointBinFilterCollection(
    filters=[
        TwoPointBinFilter.from_args(
            name1=f"lens{i}",
            measurement1=Galaxies.COUNTS,
            name2=f"lens{i}",
            measurement2=Galaxies.COUNTS,
            lower=2,
            upper=300,
        )
        for i in range(5)
    ],
    require_filter_for_all=True,
    allow_empty=True,
)
Markdown(f"```yaml\n{base_model_to_yaml(tp_collection)}\n```")
```

Equivalently, we may reduce the complexity of the code slightly and specify the use of auto-correlations only:

```{python}
tp_collection = TwoPointBinFilterCollection(
                filters=[
                    TwoPointBinFilter.from_args_auto(
                        name=f"lens{i}",
                        measurement=Galaxies.COUNTS,
                        lower=2,
                        upper=300,
                    )
                    for i in range(5)
                ],
                require_filter_for_all=True,
                allow_empty=True,
)
Markdown(f"```yaml\n{base_model_to_yaml(tp_collection)}\n```")
```

One may alternatively define the tracers directly (instead of from arguments) as `TwoPointTracerSpec` objects. 

A `TwoPointExperiment` object is able to keep track of the relevant `Factory` instances to generate the two-point configurations of the analysis (either in configuration or harmonic space) and the scale-cut/data filtering choices to evaluate a defined likelihood. 
The interpretation of the filtered lower and upper limits of the data depend on the definition of the `TwoPointExperiment` factories in either configuration or harmonic space.

With this formalism, we are able to evaluate the likelihood exactly as the previous section by defining filters to be very wide. 
Alternatively, by setting a restrictively small filtered range, we can remove data from the analysis and do so in the example below by filtering-out all galaxy clustering data. 

```{python}
from firecrown.likelihood.factories import (
    DataSourceSacc,
    TwoPointCorrelationSpace,
    TwoPointExperiment,
    TwoPointFactory,
)

tpf = base_model_from_yaml(TwoPointFactory, two_point_yaml)

two_point_experiment = TwoPointExperiment(
    two_point_factory=tpf,
    data_source=DataSourceSacc(
        sacc_data_file="../examples/des_y1_3x2pt/sacc_data.fits",
        filters=TwoPointBinFilterCollection(
            require_filter_for_all=False,
            allow_empty=True,
            filters=[
                TwoPointBinFilter.from_args_auto(
                    name=f"lens{i}",
                    measurement=Galaxies.COUNTS,
                    lower=0.5,
                    upper=300,
                )
                for i in range(5)
            ],
        ),
    ),
)

two_point_experiment_filtered = TwoPointExperiment(
    two_point_factory=tpf,
    data_source=DataSourceSacc(
        sacc_data_file="../examples/des_y1_3x2pt/sacc_data.fits",
        filters=TwoPointBinFilterCollection(
            require_filter_for_all=False,
            allow_empty=True,
            filters=[
                TwoPointBinFilter.from_args_auto(
                    name=f"lens{i}",
                    measurement=Galaxies.COUNTS,
                    lower=2999,
                    upper=3000,
                )
                for i in range(5)
            ],
        ),
    ),
)
```

The `TwoPointExperiment` objects can also be used to create likelihoods in the ready state. 
Additionally, they can be serialized into a yaml file, making it easier to share specific analysis choices with other users and collaborators.

The `yaml` below shows the first experiment.
```{python}
# | code-fold: true
Markdown(f"```yaml\n{base_model_to_yaml(two_point_experiment)}\n```")
```

The `yaml` below shows the second experiment.
```{python}
# | code-fold: true
Markdown(f"```yaml\n{base_model_to_yaml(two_point_experiment_filtered)}\n```")
```

Next, we can create likelihoods from the `TwoPointExperiment` objects and compare the loglike values.

```{python}
likelihood_tpe = two_point_experiment.make_likelihood()

params = get_default_params_map(tools, likelihood_tpe)

tools = ModelingTools()
tools.update(params)
tools.prepare()
likelihood_tpe.update(params)

likelihood_tpe_filtered = two_point_experiment_filtered.make_likelihood()

params = get_default_params_map(tools, likelihood_tpe_filtered)

tools = ModelingTools()
tools.update(params)
tools.prepare()
likelihood_tpe_filtered.update(params)

```

```{python}
# | code-fold: true
print(f"Loglike from metadata only: {likelihood.compute_loglike(tools)}")
print(f"Loglike from ready state: {likelihood_ready.compute_loglike(tools)}")
print(f"Loglike from TwoPointExperiment: {likelihood_tpe.compute_loglike(tools)}")
print(f"Loglike from filtered TwoPointExperiment: {likelihood_tpe_filtered.compute_loglike(tools)}")
```

## Controlling Integration

`TwoPointFactory` objects can include integration options, allowing control over how two-point functions are computed.  
The example below shows how to create a `TwoPointFactory` with integration options that reproduce the default behavior.  
We also create additional `TwoPointFactory` objects with alternative integration configurations: `LIMBER` with `GSL_SPLINE`, `FKEM_AUTO`, and `FKEM_L_LIMBER`.  

The lens and source redshift bin collections used for the computations are imported from the `firecrown.generators.inferred_galaxy_zdist` module:  
`LSST_Y1_LENS_BIN_COLLECTION` and `LSST_Y1_SOURCE_BIN_COLLECTION`.

```{python}
import numpy as np
from firecrown.metadata_functions import (
    make_all_photoz_bin_combinations,
    TwoPointHarmonic,
)
from firecrown.generators.inferred_galaxy_zdist import (
    LSST_Y1_LENS_BIN_COLLECTION,
    LSST_Y1_SOURCE_BIN_COLLECTION,
)

count_bins = LSST_Y1_LENS_BIN_COLLECTION.generate()
shear_bins = LSST_Y1_SOURCE_BIN_COLLECTION.generate()
all_y1_bins = count_bins[:1] + shear_bins[:1]

all_two_point_xy = make_all_photoz_bin_combinations(all_y1_bins)
ells = np.unique(
    np.concatenate((np.arange(2, 120), np.geomspace(120, 2000, 128)))
).astype(int)
all_two_point_cells = [TwoPointHarmonic(XY=xy, ells=ells) for xy in all_two_point_xy]

tpf_gsl_quad = TwoPointFactory(
    correlation_space=TwoPointCorrelationSpace.HARMONIC,
    weak_lensing_factories=tpf.weak_lensing_factories,
    number_counts_factories=tpf.number_counts_factories,
    int_options=ClIntegrationOptions(
        method=ClIntegrationMethod.LIMBER, limber_method=ClLimberMethod.GSL_QAG_QUAD
    ),
)

tpf_gsl_spline = TwoPointFactory(
    correlation_space=TwoPointCorrelationSpace.HARMONIC,
    weak_lensing_factories=tpf.weak_lensing_factories,
    number_counts_factories=tpf.number_counts_factories,
    int_options=ClIntegrationOptions(
        method=ClIntegrationMethod.LIMBER, limber_method=ClLimberMethod.GSL_SPLINE
    ),
)

tpf_fkem_auto = TwoPointFactory(
    correlation_space=TwoPointCorrelationSpace.HARMONIC,
    weak_lensing_factories=tpf.weak_lensing_factories,
    number_counts_factories=tpf.number_counts_factories,
    int_options=ClIntegrationOptions(
        method=ClIntegrationMethod.FKEM_AUTO, limber_method=ClLimberMethod.GSL_QAG_QUAD
    ),
)

tpf_fkem_l_limber = TwoPointFactory(
    correlation_space=TwoPointCorrelationSpace.HARMONIC,
    weak_lensing_factories=tpf.weak_lensing_factories,
    number_counts_factories=tpf.number_counts_factories,
    int_options=ClIntegrationOptions(
        method=ClIntegrationMethod.FKEM_L_LIMBER,
        limber_method=ClLimberMethod.GSL_QAG_QUAD,
        l_limber=50,
    ),
)

tpf_fkem_l_limber_max = TwoPointFactory(
    correlation_space=TwoPointCorrelationSpace.HARMONIC,
    weak_lensing_factories=tpf.weak_lensing_factories,
    number_counts_factories=tpf.number_counts_factories,
    int_options=ClIntegrationOptions(
        method=ClIntegrationMethod.FKEM_L_LIMBER,
        limber_method=ClLimberMethod.GSL_QAG_QUAD,
        l_limber=2100,
    ),
)

two_points_gsl_quad = tpf_gsl_quad.from_metadata(all_two_point_cells)
two_points_gsl_spline = tpf_gsl_spline.from_metadata(all_two_point_cells)
two_points_fkem_auto = tpf_fkem_auto.from_metadata(all_two_point_cells)
two_points_fkem_l_limber = tpf_fkem_l_limber.from_metadata(all_two_point_cells)
two_points_fkem_l_limber_max = tpf_fkem_l_limber_max.from_metadata(all_two_point_cells)

```

Now we plot the relative differences between each integration method and the most accurate (`FKEM` applied to all ells), highlighting the impact of different integration choices on the two-point functions.

```{python}
# | label: fig-fz
# | fig-cap: Relative difference to default behavior
# | fig-cap-location: margin
# | code-fold: true
from plotnine import *  # bad form in programs, but seems OK for plotnine
import pandas as pd

two_point0_gsl_quad = two_points_gsl_quad[0]
two_point0_gsl_spline = two_points_gsl_spline[0]
two_point0_fkem_auto = two_points_fkem_auto[0]
two_point0_fkem_l_limber = two_points_fkem_l_limber[0]
two_point0_fkem_l_limber_max = two_points_fkem_l_limber_max[0]
meta0 = all_two_point_cells[0]

two_point0_gsl_quad.update(get_default_params_map(two_point0_gsl_quad))
two_point0_gsl_spline.update(get_default_params_map(two_point0_gsl_spline))
two_point0_fkem_auto.update(get_default_params_map(two_point0_fkem_auto))
two_point0_fkem_l_limber.update(get_default_params_map(two_point0_fkem_l_limber))
two_point0_fkem_l_limber_max.update(
    get_default_params_map(two_point0_fkem_l_limber_max)
)

tv0_gsl_quad = two_point0_gsl_quad.compute_theory_vector(tools)
tv0_gsl_spline = two_point0_gsl_spline.compute_theory_vector(tools)
tv0_fkem_auto = two_point0_fkem_auto.compute_theory_vector(tools)
tv0_fkem_l_limber = two_point0_fkem_l_limber.compute_theory_vector(tools)
tv0_fkem_l_limber_max = two_point0_fkem_l_limber_max.compute_theory_vector(tools)

tmp = np.abs(tv0_gsl_spline / tv0_fkem_l_limber_max - 1.0)
data_gsl_spline = pd.DataFrame(
    {
        "ell": two_point0_gsl_spline.ells[tmp > 0.0],
        "rel-diff": tmp[tmp > 0.0],
        "bin-x": meta0.XY.x.bin_name,
        "bin-y": meta0.XY.y.bin_name,
        "measurement": meta0.get_sacc_name(),
        "integration": "GSL SPLINE",
    }
)

tmp = np.abs(tv0_gsl_quad / tv0_fkem_l_limber_max - 1.0)
data_gsl_quad = pd.DataFrame(
    {
        "ell": two_point0_gsl_quad.ells[tmp > 0.0],
        "rel-diff": tmp[tmp > 0.0],
        "bin-x": meta0.XY.x.bin_name,
        "bin-y": meta0.XY.y.bin_name,
        "measurement": meta0.get_sacc_name(),
        "integration": "GSL QAG_QUAD",
    }
)

tmp = np.abs(tv0_fkem_auto / tv0_fkem_l_limber_max - 1.0)
data_fkem_auto = pd.DataFrame(
    {
        "ell": two_point0_fkem_auto.ells[tmp > 0.0],
        "rel-diff": tmp[tmp > 0.0],
        "bin-x": meta0.XY.x.bin_name,
        "bin-y": meta0.XY.y.bin_name,
        "measurement": meta0.get_sacc_name(),
        "integration": "FKEM AUTO",
    }
)

tmp = np.abs(tv0_fkem_l_limber / tv0_fkem_l_limber_max - 1.0)
data_fkem_l_limber = pd.DataFrame(
    {
        "ell": two_point0_fkem_l_limber.ells[tmp > 0.0],
        "rel-diff": tmp[tmp > 0.0],
        "bin-x": meta0.XY.x.bin_name,
        "bin-y": meta0.XY.y.bin_name,
        "measurement": meta0.get_sacc_name(),
        "integration": "FKEM l-limber (50)",
    }
)

data = pd.concat([data_gsl_spline, data_gsl_quad, data_fkem_auto, data_fkem_l_limber])

# Now we can generate the plot.
(
    ggplot(data, aes("ell", "rel-diff"))
    + geom_line()
    + labs(x=r"$\ell$", y=r"$|C^X_ell/C^\mathrm{gsl quad}_\ell - 1|$")
    + scale_x_log10()
    + scale_y_log10()
    + doc_theme()
    + facet_wrap("integration")
    + theme(figure_size=(10, 6))
)

```

